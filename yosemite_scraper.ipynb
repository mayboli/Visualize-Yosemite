{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import time\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    print('COMMENCING DATA SCRAPE FOR YOSEMITE NATIONAL PARK')\n",
    "    print('-------------------------------------------------------')\n",
    "    \n",
    "    # dictionary to hold final scraped data\n",
    "    yosemite_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING DATA SCRAPE FOR ECONOMIC BENEFITS FROM NEWS ARTICLES\n"
     ]
    }
   ],
   "source": [
    "    # initialize browser\n",
    "    executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    \n",
    "    print('INITIALIZING DATA SCRAPE FOR ECONOMIC BENEFITS FROM NEWS ARTICLES')\n",
    "    \n",
    "    # URL of yosemite articles page to be scraped\n",
    "    url = 'https://www.nps.gov/yose/learn/news/newsreleases.htm'\n",
    "    browser.visit(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # empty lists to hold raw scraped data\n",
    "    article_links = []\n",
    "    headlines = []\n",
    "    article_contents = []\n",
    "\n",
    "    # empty lists that will hold cleaned scraped data\n",
    "    years = []\n",
    "    amounts = []\n",
    "    job_counts = []\n",
    "    visitor_counts = []\n",
    "\n",
    "    # empty list to hold final scraped data\n",
    "    economic_benefits = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBTAINED ECONOMIC BENEFITS\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    # go through pages 1-33 and find links of targeted articles\n",
    "    for x in range(1, 34):\n",
    "    \n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        article_snippets = soup.find_all('li', class_='ListingList-item')\n",
    "        substring = 'Economic Benefit'\n",
    "    \n",
    "        for article_snippet in article_snippets:\n",
    "            snippet_headline = article_snippet.find('h3', class_='ListingResults-title').text\n",
    "        \n",
    "            if substring in snippet_headline:\n",
    "            \n",
    "                end_link = article_snippet.find('a')['href']\n",
    "                article_link = 'https://www.nps.gov' + end_link\n",
    "                article_links.append(article_link)\n",
    "            \n",
    "        browser.click_link_by_text('Next ')\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print('OBTAINED ECONOMIC BENEFITS')\n",
    "    print('-------------------------------------------------------')\n",
    "    #article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # visit each article link and extract content\n",
    "    for article_link in article_links:\n",
    "\n",
    "        browser.visit(article_link)\n",
    "        article_html = browser.html\n",
    "        article_soup = BeautifulSoup(article_html, 'html.parser')\n",
    "\n",
    "        headline = article_soup.find('div', class_='ContentHeader').text\n",
    "        headline = headline.replace('\\n', '')\n",
    "        headlines.append(headline)\n",
    "\n",
    "        article_content = article_soup.find('div', class_='ArticleTextGroup').text\n",
    "        article_contents.append(article_content)\n",
    "\n",
    "    #headlines\n",
    "    #article_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # loop through headlines and extract economic benefit $ amount (in millions)\n",
    "    for headline in headlines:\n",
    "        headline_split = headline.split('$')[1]\n",
    "        amount = headline_split[:3]\n",
    "        amounts.append(amount)\n",
    "    #amounts\n",
    "\n",
    "    # loop through article contents and extract year, job count, and visitor count\n",
    "    for article_content in article_contents:\n",
    "        year_split = article_content.split('Park in ')[1]\n",
    "        year = year_split[:4]\n",
    "        years.append(year)\n",
    "\n",
    "        job_split = article_content.split('supported ')[1]\n",
    "        job_count = job_split[:5]\n",
    "        if ',' in job_count:\n",
    "            job_count = job_count.replace(',', '')\n",
    "            job_counts.append(job_count)\n",
    "        elif ' ' in job_count:\n",
    "            job_count = job_count.replace(' ', '')\n",
    "            job_counts.append(job_count)\n",
    "        else: \n",
    "            job_counts.append(job_count)\n",
    "\n",
    "        visitor_split = article_content.split('shows that')[1]\n",
    "        visitor_count = visitor_split[:10]\n",
    "        visitor_count = visitor_count.replace(',', '').replace('\\xa0', '').replace(' ', '')\n",
    "        visitor_counts.append(visitor_count)\n",
    "\n",
    "    #years\n",
    "    #job_counts\n",
    "    #visitor_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # append extract information into a dictionary that will be uploaded into mongodb\n",
    "    economic_benefits.append({'years': years,\n",
    "                        'amounts': amounts,\n",
    "                       'job_counts': job_counts,\n",
    "                       'visitor_counts': visitor_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #append missing 2015 data\n",
    "    economic_benefits[0]['years'].insert(2, '2015')\n",
    "    economic_benefits[0]['amounts'].insert(2, '594')\n",
    "    economic_benefits[0]['job_counts'].insert(2, '6890')\n",
    "    economic_benefits[0]['visitor_counts'].insert(2, '4150217')\n",
    "\n",
    "    #economic_benefits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yosemite_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f5464ffe6e23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myosemite_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'economic_benefits'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meconomic_benefits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'yosemite_data' is not defined"
     ]
    }
   ],
   "source": [
    "    yosemite_data['economic_benefits'] = economic_benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
